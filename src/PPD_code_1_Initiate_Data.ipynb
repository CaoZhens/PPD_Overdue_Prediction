{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "title = 'PPD'\n",
    "path = '../sources/data/PPD-First-Round-Data'\n",
    "icy = 'target'\n",
    "\n",
    "def Del_string(xstr):\n",
    "    xstrc = xstr.strip().strip(u'市').strip(u'省')\n",
    "    if(xstrc == ''):\n",
    "        xstrc = np.nan\n",
    "    return(xstrc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Data\n",
    "\n",
    "Read data from orginal data files, save them into database which is easier to reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取初赛轮（First Round）中，TrainSet & TestSet中的Master数据\n",
    "#### TrainSet (30000, 227)  |  TestSet (19999, 226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'na_values': [-1], 'parse_dates': ['ListingInfo'], 'converters': {'UserInfo_2': <function Del_string at 0x119d82d70>, 'UserInfo_7': <function Del_string at 0x119d82d70>, 'UserInfo_4': <function Del_string at 0x119d82d70>, 'UserInfo_19': <function Del_string at 0x119d82d70>, 'UserInfo_8': <function Del_string at 0x119d82d70>, 'UserInfo_9': <function Del_string at 0x119d82d70>, 'UserInfo_20': <function Del_string at 0x119d82d70>}, 'index_col': 0, 'encoding': 'GB18030'}\n",
      "(30000, 227)\n",
      "(19999, 226)\n"
     ]
    }
   ],
   "source": [
    "# 读取初赛trainset和testset master\n",
    "# 备注:原始数据中某些属性的取值已经是-1了，但从逻辑上讲，仍应表示缺失值\n",
    "par_csv = dict(index_col = 0, encoding = 'GB18030', parse_dates = [\"ListingInfo\"], na_values = [-1], \n",
    "               converters = dict(zip(*[[\"UserInfo_{}\".format(i) for i in [2,4,7,8,9,19,20]], [Del_string]*7])))\n",
    "print par_csv\n",
    "\n",
    "dat_master = pd.read_csv('{}/TrainingSet/PPD_FR_TrainSet_Master.csv'.format(path), **par_csv)\n",
    "dav_master = pd.read_csv('{}/TestSet/PPD_FR_TestSet_Master.csv'.format(path), **par_csv)\n",
    "print dat_master.shape\n",
    "print dav_master.shape\n",
    "da = pd.concat([dat_master, dav_master])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fanghan/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py:280: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->unicode,key->axis0] [items->None]\n",
      "\n",
      "  f(store)\n",
      "/Users/fanghan/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py:280: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->unicode,key->block0_items] [items->None]\n",
      "\n",
      "  f(store)\n",
      "/Users/fanghan/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py:280: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->unicode,key->block1_items] [items->None]\n",
      "\n",
      "  f(store)\n",
      "/Users/fanghan/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py:280: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->unicode,key->block2_items] [items->None]\n",
      "\n",
      "  f(store)\n",
      "/Users/fanghan/anaconda/lib/python2.7/site-packages/pandas/core/generic.py:1299: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->[u'Education_Info2', u'Education_Info3', u'Education_Info4', u'Education_Info6', u'Education_Info7', u'Education_Info8', u'UserInfo_19', u'UserInfo_2', u'UserInfo_20', u'UserInfo_22', u'UserInfo_23', u'UserInfo_24', u'UserInfo_4', u'UserInfo_7', u'UserInfo_8', u'UserInfo_9', u'WeblogInfo_19', u'WeblogInfo_20', u'WeblogInfo_21']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n",
      "/Users/fanghan/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py:280: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->unicode,key->block3_items] [items->None]\n",
      "\n",
      "  f(store)\n"
     ]
    }
   ],
   "source": [
    "np.save('../sources/deal/PPD-Data-Saving/{}_irt.npy'.format(title), list(dat_master.index))\n",
    "np.save('../sources/deal/PPD-Data-Saving/{}_irv.npy'.format(title), list(dav_master.index))\n",
    "da.to_hdf('../sources/deal/PPD-Data-Saving/{}_da.h5'.format(title), key = 'da', complib = 'zlib', complevel = 1, mode = 'w')\n",
    "\n",
    "# dat_master.head()\n",
    "# dav_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取First Round中，TrainSet & TestSet中的loginfo & userupdate数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Idx Listinginfo1  LogInfo1  LogInfo2   LogInfo3\n",
      "0  10001   2014-03-05       107         6 2014-02-20\n",
      "1  10001   2014-03-05       107         6 2014-02-23\n",
      "2  10001   2014-03-05       107         6 2014-02-24\n",
      "3  10001   2014-03-05       107         6 2014-02-25\n",
      "4  10001   2014-03-05       107         6 2014-02-27\n",
      "\n",
      "\n",
      "          LogInfo1  LogInfo2\n",
      "Id Time                     \n",
      "3  -67.0         1         0\n",
      "   -67.0         1         1\n",
      "   -67.0         1         4\n",
      "   -67.0         2         1\n",
      "   -67.0         4         1\n",
      "\n",
      "\n",
      "     Idx ListingInfo1    UserupdateInfo1 UserupdateInfo2\n",
      "0  10001   2014-03-05       _educationid      2014-02-20\n",
      "1  10001   2014-03-05         _hasbuycar      2014-02-20\n",
      "2  10001   2014-03-05    _lastupdatedate      2014-02-20\n",
      "3  10001   2014-03-05  _marriagestatusid      2014-02-20\n",
      "4  10001   2014-03-05       _mobilephone      2014-02-20\n",
      "\n",
      "\n",
      "            UserupdateInfo1\n",
      "Id Time                    \n",
      "3  -67.0       _educationid\n",
      "   -67.0         _hasbuycar\n",
      "   -67.0          _idnumber\n",
      "   -67.0    _lastupdatedate\n",
      "   -67.0  _marriagestatusid\n"
     ]
    }
   ],
   "source": [
    "dat_loginfo = pd.read_csv('{}/TrainingSet/PPD_FR_TrainSet_LogInfo.csv'.format(path), parse_dates = ['Listinginfo1','LogInfo3'])\n",
    "dav_loginfo = pd.read_csv('{}/TestSet/PPD_FR_TestSet_LogInfo.csv'.format(path), parse_dates = ['Listinginfo1','LogInfo3'])\n",
    "dah_loginfo = pd.concat([dat_loginfo, dav_loginfo])\n",
    "print dah_loginfo.head()\n",
    "dah1 = dah_loginfo.assign(Id=dah_loginfo['Idx'], \\\n",
    "                          Time = (dah_loginfo['LogInfo3']-dah_loginfo['Listinginfo1']).astype('timedelta64[D]'))\\\n",
    "           .set_index(['Id','Time']).drop(['Idx','Listinginfo1','LogInfo3'], axis=1).sort_index()\n",
    "print '\\n'\n",
    "print dah1.head()\n",
    "\n",
    "dat_uu = pd.read_csv('{}/TrainingSet/PPD_FR_TrainSet_Userupdate.csv'.format(path), \\\n",
    "                     parse_dates = ['ListingInfo1','UserupdateInfo2'],\\\n",
    "                     converters = {'UserupdateInfo1': lambda x: x.lower()})\n",
    "dav_uu = pd.read_csv('{}/TestSet/PPD_FR_TestSet_Userupdate.csv'.format(path), \\\n",
    "                     parse_dates = ['ListingInfo1','UserupdateInfo2'],\\\n",
    "                     converters = {'UserupdateInfo1': lambda x: x.lower()})\n",
    "dah_uu = pd.concat([dat_uu, dav_uu])\n",
    "print '\\n'\n",
    "print dah_uu.head()\n",
    "dah2 = dah_uu.assign(Id=dah_uu['Idx'], \\\n",
    "              Time=(dah_uu['UserupdateInfo2']-dah_uu['ListingInfo1']).astype('timedelta64[D]'))\\\n",
    "      .set_index(['Id','Time']).drop(['Idx','ListingInfo1','UserupdateInfo2'], axis=1).sort_index()\n",
    "print '\\n'\n",
    "print dah2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dah1.to_hdf('../sources/deal/PPD-Data-Saving/{}_dah1.h5'.format(title), key = 'dah', complib = 'zlib', complevel = 1, mode = 'w')\n",
    "dah2.to_hdf('../sources/deal/PPD-Data-Saving/{}_dah2.h5'.format(title), key = 'dah', complib = 'zlib', complevel = 1, mode = 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Data\n",
    "Load data from database, then concatenate and summerize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49999, 227)\n",
      "(966431, 2)\n",
      "(621295, 1)\n"
     ]
    }
   ],
   "source": [
    "da = pd.read_hdf('../sources/deal/PPD-Data-Saving/{}_da.h5'.format(title), key = 'da')\n",
    "dah1 = pd.read_hdf('../sources/deal/PPD-Data-Saving/{}_dah1.h5'.format(title), key = 'dah')\n",
    "dah2 = pd.read_hdf('../sources/deal/PPD-Data-Saving/{}_dah2.h5'.format(title), key = 'dah')\n",
    "irt = np.load('../sources/deal/PPD-Data-Saving/{}_irt.npy'.format(title))\n",
    "irv = np.load('../sources/deal/PPD-Data-Saving/{}_irv.npy'.format(title))\n",
    "print da.shape\n",
    "print dah1.shape\n",
    "print dah2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Concatenate Historical Data with Master Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 352)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Preprocess_historicalData(dah, iccat, name, da):\n",
    "    grp1 = dah.reset_index()[[\"Id\", \"Time\"]].groupby(\"Id\")\n",
    "    grp2 = dah.groupby(level = [\"Id\", \"Time\"]).first().reset_index()[[\"Id\", \"Time\"]].groupby(\"Id\")\n",
    "    dahc1 = pd.concat([grp1.first(), grp1.count()], axis = 1, ignore_index = True).rename(columns = {0:\"FirstTime\", 1:\"Count\"})\n",
    "    dahc1 = dahc1.assign(DayFreq = grp2.count()[\"Time\"]/(1-dahc1[\"FirstTime\"])).loc[da.index]\n",
    "    dahc2 = dah.reset_index().groupby([\"Id\"]+iccat).count().unstack(iccat)[\"Time\"].loc[da.index]\n",
    "    dahc = pd.concat([dahc1, pd.DataFrame({\"Cats\": dahc2.notnull().sum(axis = 1)}), dahc2], axis = 1).fillna(0)\n",
    "    dahc.columns = [\"{}_{}\".format(name, x) for x in dahc.columns]\n",
    "    return(dahc)\n",
    "\n",
    "dah1c = Preprocess_historicalData(dah1, list(dah1.columns), 'Log', da)\n",
    "dah2c = Preprocess_historicalData(dah2, list(dah2.columns), 'Userupdate', da)\n",
    "# print dah1c.head()\n",
    "# print dah2c.head()\n",
    "da = pd.concat([da, dah1c, dah2c], axis = 1)\n",
    "da.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
